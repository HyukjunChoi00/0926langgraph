import json
import asyncio
from typing import Annotated
from typing import TypedDict, Annotated, List, Dict, Optional
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.tools import tool
from playwright.async_api import async_playwright
import os
os.environ["GOOGLE_API_KEY"] = ''

# ìœˆë„ìš° ì‚¬ìš©ìì˜ ê²½ìš° ì•„ë˜ ìœˆë„ìš° í˜¸í™˜ì„± ì •ì±… ë³€ê²½ ì½”ë“œ
import sys
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

#############################  íŒ¨í‚¤ì§€ import

############# streamlit
import streamlit as st

st.title('ì´ë¦„ í•™ë²ˆ ê²€ìƒ‰ Agent')

#################


llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)


##### ê° ë¬¸ë²• ì„¤ëª… #####

# TypedDict : ë” ì—„ê²©í•œ íƒ€ì… ê²€ì‚¬, íƒ€ì… íŒíŠ¸ ì œê³µ
# Annotated : íƒ€ì… íŒíŠ¸ì— ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€  ì˜ˆ) Annotated[Type, metadata1, metadata2, ...]
# Annotated : LangGraphì—ì„œëŠ” Annotatedë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ë³„í•œ ë™ì‘ì„ ì •ì˜
# add_messages : ë©”ì‹œì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

##################################################################

# ìƒíƒœ ì •ì˜

class State(TypedDict):
    query: str
    messages: Annotated[list, add_messages]
    extracted_keyword: str
    expanded_keywords: list[str]
    search_results: list[dict]
    answer: str

##################################################################

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (ì•„ì§ ë…¸ë“œë¡œ ì •ì˜í•˜ì§„ ì•Šì•˜ê³ , chainì„ ë§Œë“¤ì–´ë‘ )

def build_keyword_extraction_chain():
    prompt_template = ChatPromptTemplate.from_messages(
        [("system", "ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ í•µì‹¬ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” Assistantì´ë‹¤."),
         ("user", "ë‹¤ìŒ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ë¼. ë°˜ë“œì‹œ ì˜ë¬¸ í‚¤ì›Œë“œë¡œ ì œê³µí•˜ë¼.\n"
                  "ì˜ˆì‹œ:\n"
                  "- 'NVIDIA ì£¼ì‹ ë™í–¥ ì¡°ì‚¬ì¢€ í•´ì¤˜' â†’ 'NVIDIA'\n"
                  "- 'í…ŒìŠ¬ë¼ ì£¼ê°€ ì „ë§ì´ ì–´ë•Œ?' â†’ 'Tesla'\n"
                  "- 'ì• í”Œ ì•„ì´í° ìµœì‹  ì†Œì‹ ì•Œë ¤ì¤˜' â†’ 'Apple'\n"
                  "- 'AI ê¸°ìˆ  ë°œì „ í˜„í™©' â†’ 'AI'\n"
                  "ì§ˆë¬¸: {query}\n\n"
                  "ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì¤˜:\n"
                  '{{"extracted_keyword": "ì¶”ì¶œëœ_í‚¤ì›Œë“œ"}}\n'
                  "í•µì‹¬ í‚¤ì›Œë“œ:")]
    )
    
    return prompt_template | llm

##################################################################

# ì¿¼ë¦¬ í™•ì¥ í•¨ìˆ˜ (chain)

def build_query_expansion_chain():
    prompt_template = ChatPromptTemplate.from_messages(
        [("system", "ë„ˆëŠ” í‚¤ì›Œë“œí˜• Query Expansion Assistantì´ë‹¤. ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ì‚°ì—…, ê¸°ìˆ , ì‹œì¥ í‚¤ì›Œë“œë¡œ í™•ì¥í•´ì•¼ í•œë‹¤."),
         ("user", "ë‹¤ìŒ í‚¤ì›Œë“œë¥¼ í™•ì¥í•˜ë¼:\n"
                  "ì›ë³¸ í‚¤ì›Œë“œ: {query}\n"
                  "í™•ì¥í•  ê°œìˆ˜: {n}ê°œ\n\n"
                  "í™•ì¥ ê·œì¹™:\n"
                  "- Tesla â†’ electric vehicle, EV, battery\n"
                  "- NVIDIA â†’ AI, GPU, semiconductor\n"
                  "- Apple â†’ iPhone, technology, consumer electronics\n"
                  "- Microsoft â†’ cloud computing, software, Azure\n\n"
                  "'{query}' í‚¤ì›Œë“œì— ëŒ€í•´ ê´€ë ¨ ì‚°ì—…/ê¸°ìˆ  í‚¤ì›Œë“œ {n}ê°œë¥¼ ì˜ë¬¸ìœ¼ë¡œ ìƒì„±í•˜ë¼.\n\n"
                  "ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ë¼ (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ í¬í•¨ ê¸ˆì§€):\n"
                  '{{"expanded_search_query_list": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"]}}\n')]
    )
    
    return prompt_template | llm

##################################################################

# LLMì„ ì´ìš©í•œ ë‹µë³€ ìƒì„±
# ì•„ë˜ ì½”ë“œ ì¤‘ state.get("search_results ... )ë¥¼ í†µí•´, ì´ì „ ë…¸ë“œì— ì €ì¥ëœ ë°˜í™˜ê°’ì¸ search_results ê°’ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ.
# ê°€ì ¸ì˜¨ ê°’ì„ contextë¼ëŠ” ë³€ìˆ˜ì— ì €ì¥
# contextë¥¼ promptì— ì‚½ì…
# ì¦‰, LLMì´ contextë¥¼ ë³´ê³  ë‹µì„ í•  ìˆ˜ ìˆìŒ.
# ë‹µë³€ì„ markdown ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰í•˜ì—¬, ì´í›„ Streamlitì—ì„œ ë³¼ ë‹µë³€ì´ ë” êµ¬ì¡°ë„ ìˆê²Œ ë³´ì´ë„ë¡

def generate_response(state):
    context = state.get("search_results", "")
    
    curr_human_turn = HumanMessage(content=f"ì§ˆë¬¸: {state['query']}\n"
                            f"ê²€ìƒ‰ ê²°ê³¼:\n```\n{context}```"
                             "\n---\n"
                             "ì‘ë‹µì€ markdownì„ ì´ìš©í•´ ë¦¬í¬íŠ¸ ìŠ¤íƒ€ì¼ë¡œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•´ë¼. "
                             "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì˜ ì˜ë„ì— ë§ëŠ” ì •ë‹µ ë¶€ë¶„ì„ ê°•ì¡°í•´ë¼.")
    messages = state["messages"] + [curr_human_turn]
    response = llm.invoke(messages)

    return {"messages": [*messages, response],
            "answer": response.content}

##################################################################

def parse_json_response(response) -> dict:
    """JSON ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜"""

    # AIMessage ê°ì²´ì—ì„œ content ì¶”ì¶œ
    content = str(getattr(response, 'content', response))

    # JSON ë¸”ë¡ ì°¾ê¸°
    if '```json' in content:
        start = content.find('```json') + 7
        end = content.find('```', start)
        json_str = content[start:end].strip()
    elif '{' in content and '}' in content:
        start = content.find('{')
        end = content.rfind('}') + 1
        json_str = content[start:end]
    else:
        return {"extracted_keyword": "", "expanded_search_query_list": []}

    try:
        return json.loads(json_str)
    except json.JSONDecodeError:
        return {"extracted_keyword": "", "expanded_search_query_list": []}

##################################################### ë…¸ë“œ ìƒì„± #################################################################

# í‚¤ì›Œë“œ ì¶”ì¶œ

def extract_keyword(state):
    """í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë…¸ë“œ"""
    print(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘: {state['query']}")
    
    keyword_extraction_chain = build_keyword_extraction_chain()
    original_query = state["query"]
    response = keyword_extraction_chain.invoke({"query": original_query})
    parsed_response = parse_json_response(response)
    
    extracted_keyword = parsed_response.get("extracted_keyword", "")
    print(f"âœ… ì¶”ì¶œëœ í‚¤ì›Œë“œ: {extracted_keyword}")
    
    return {"extracted_keyword": extracted_keyword}

##################################################################

def query_expansion(state):
    """ì¿¼ë¦¬ í™•ì¥ ë…¸ë“œ"""
    print(f"ğŸ”„ ì¿¼ë¦¬ í™•ì¥ ì¤‘: {state['extracted_keyword']}")
    
    query_expansion_chain = build_query_expansion_chain()
    extracted_keyword = state["extracted_keyword"]
    response = query_expansion_chain.invoke({"query": extracted_keyword, "n": 2})
    parsed_response = parse_json_response(response)
    
    expanded_keywords = parsed_response.get("expanded_search_query_list", [])
    # ì›ë³¸ í‚¤ì›Œë“œë„ í¬í•¨
    all_keywords = [extracted_keyword] + expanded_keywords
    print(f"âœ… í™•ì¥ëœ í‚¤ì›Œë“œ: {all_keywords}")
    
    return {"expanded_keywords": all_keywords}

#################################################################

# ê²€ìƒ‰ ë…¸ë“œ
# í™•ì¥ëœ í‚¤ì›Œë“œ ëª©ë¡ì„ ë°›ê³ 
# ê° í‚¤ì›Œë“œì— ëŒ€í•œ ê²€ìƒ‰ì„ ìˆ˜í–‰
# ì´í›„ ê²°ê³¼ê°’ì„ all_resultsì— ì •ë¦¬ í›„, search resultsë¡œ ë°˜í™˜í™˜

async def search_news(state):
    """ë‰´ìŠ¤ ê²€ìƒ‰ ë…¸ë“œ"""
    print(f"ğŸ“° ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
    
    expanded_keywords = state.get("expanded_keywords", [])  # í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ì¸ ë¹ˆ ë¦¬ìŠ¤íŠ¸ [] ë°˜í™˜. KeyError ë°©ì§€.
    all_results = []
    for query in expanded_keywords:
        results = await scrape_articles_with_content(query)
        results = results['search_results']

        for i in results :
            title = i['title']
            url = i['url']
            content = i['content']
            all_results.extend([{
                    "title": title,
                    "url": url,
                    "content": content,
                    "search_query": query
                }])
    

    
    return {"search_results": all_results}



async def scrape_articles_with_content(query: str, max_articles: int = 3) -> str:
    """
    Econotimesì—ì„œ ê´€ë ¨ ê¸°ì‚¬ ì œëª©, URL, ë³¸ë¬¸ì„ ìŠ¤í¬ë©í•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜
    ì‚¬ìš©ì íŠ¹ì • ì¢…ëª©ì— ëŒ€í•œ ë™í–¥ ë¶„ì„ ë“±ì„ ìš”ì²­í•  ë•Œ ì´ ë„êµ¬ë¥¼ ì´ìš©í•´ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    
    Args:
        query: ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì˜ˆ: tesla, apple, bitcoin ë“±)
        max_articles: ì¶”ì¶œí•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 3)
    
    Returns:
        ê¸°ì‚¬ ì •ë³´ê°€ í¬í•¨ëœ JSON í˜•íƒœì˜ ë¬¸ìì—´
    """
    print(f"ğŸš€ Econotimesì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")
    
    output_list = []
    
    try:
        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            # ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
            search_url = f"https://econotimes.com/search?v={query}&search="
            await page.goto(search_url)
            await asyncio.sleep(2)
            
            # XPathë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë“  ê¸°ì‚¬ ì œëª© ìš”ì†Œ ì°¾ê¸°
            general_xpath = '//*[@id="archivePage"]/div/div[2]/div/p[1]/a'
            elements = await page.locator(f"xpath={general_xpath}").all()
            
            
            if not elements:
                await browser.close()
                return f"'{query}'ì— ëŒ€í•œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ ê¸°ì‚¬ ì²˜ë¦¬
            for i, element in enumerate(elements[:max_articles], 1):
                try:
                    # ê¸°ì‚¬ ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                    title = await element.text_content()
                    href = await element.get_attribute('href')
                    
                    if title and href:
                        title = title.strip()
                        full_url = f"https://econotimes.com{href}" if href.startswith('/') else href
                        
                        print(f"{i}. {title}")
                        
                        # ìƒˆ íƒ­ì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
                        article_page = await browser.new_page()
                        try:
                            await article_page.goto(full_url)
                            await asyncio.sleep(2)
                            
                            # ë³¸ë¬¸ ì¶”ì¶œ
                            article_xpath = '//*[@id="view"]/div[2]/div[3]/article'
                            article_content = await article_page.locator(f"xpath={article_xpath}").text_content()
                            
                            if article_content:
                                article_content = article_content.strip()
                                # ë³¸ë¬¸ì´ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ
                                content_preview = article_content[:800] + "..." if len(article_content) > 800 else article_content
                            else:
                                content_preview = "ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                            
                            output_list.append({
                                'number': i,
                                'title': title,
                                'url': full_url,
                                'content': content_preview
                            })
                            
                            
                            
                        except Exception as e:
                            print(f"   âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                            output_list.append({
                                'number': i,
                                'title': title,
                                'url': full_url,
                                'content': 'ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨'
                            })
                        finally:
                            await article_page.close()
                
                except Exception as e:
                    print(f"{i}. âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            await browser.close()
            
            if output_list:
                return {"search_results": output_list}

            else:
                return f"'{query}' ê¸°ì‚¬ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                
    except Exception as e:
        return f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


####################################################### ì›Œí¬í”Œë¡œìš° êµ¬ì„± #################################################################


# ì›Œí¬í”Œë¡œìš° êµ¬ì„±
workflow = StateGraph(State)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("extract_keyword", extract_keyword)
workflow.add_node("query_expansion", query_expansion)
workflow.add_node("search_news", search_news)
workflow.add_node("generate_response", generate_response)

# ì—£ì§€ ì—°ê²°
workflow.add_edge(START, "extract_keyword")
workflow.add_edge("extract_keyword", "query_expansion")
workflow.add_edge("query_expansion", "search_news")
workflow.add_edge("search_news", "generate_response")
workflow.add_edge("generate_response", END)

# ê·¸ë˜í”„ ì»´íŒŒì¼
graph = workflow.compile()

#############################################################

# LangGraphì˜ astream() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸°ì ìœ¼ë¡œ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°

async def async_stream(query):
    # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
    async for event in graph.astream({"query": query}, debug=True):
        yield event


# async_streamì´ë¼ëŠ” ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¥¼ ì‹¤í–‰í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ ëª¨ì•„ì„œ ë°˜í™˜í•˜ëŠ” ì—­í• 
# syncio.new_event_loop()ë¥¼ í˜¸ì¶œí•˜ì—¬ ìƒˆë¡œìš´ asyncio ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ìƒì„±
# Streamlitê³¼ ê°™ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ” í™˜ê²½ì—ì„œ asyncio.run()ì„ ì§ì ‘ ì‚¬ìš©í–ˆì„ ë•Œ ë°œìƒí•˜ëŠ” RuntimeErrorë¥¼ í”¼í•˜ê¸° ìœ„í•œ ë°©ë²•
# async def gather_events(): ë¹„ë™ê¸° ì œë„ˆë ˆì´í„° genì„ ë°˜ë³µì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ events ë¦¬ìŠ¤íŠ¸ì— ìˆ˜ì§‘í•˜ëŠ” ì—­í• 


def run_async_stream(query):
    # Streamlit ë™ê¸° í•¨ìˆ˜ì—ì„œ ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” í—¬í¼
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    gen = async_stream(query)
    
    async def gather_events():
        events = []
        async for event in gen:
            events.append(event)
        return events

    return loop.run_until_complete(gather_events())




if query := st.chat_input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
    st.subheader(f"ğŸ” ê²€ìƒ‰: {query}")
    st.subheader("ğŸ¤– ë‹µë³€")
    with st.spinner("ë‹µë³€ ìƒì„±ì¤‘..."):
        events = run_async_stream(query)

        # ë°›ì€ ì´ë²¤íŠ¸ë“¤ì„ í™”ë©´ì— ì¶œë ¥
        for event in events:
            for k, v in event.items():
                if k == 'extract_keyword':
                    with st.container():
                        st.write("### ğŸ”‘ ì¶”ì¶œëœ í•µì‹¬ í‚¤ì›Œë“œ")
                        st.markdown(f"**ì›ë³¸ ì§ˆë¬¸:** {query}")
                        st.markdown(f"**ì¶”ì¶œëœ í‚¤ì›Œë“œ:** {v['extracted_keyword']}")
                        
                elif k == 'query_expansion':
                    with st.container():
                        st.write("### ğŸ” í™•ì¥ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸")
                        expanded_query_md = '\n'.join([f"- {q}" for q in v['expanded_keywords']])
                        st.markdown(expanded_query_md)
                        
                elif k == 'search_news':
                    with st.expander("ğŸ“° ê²€ìƒ‰ëœ ë‰´ìŠ¤ (EconoTimes)"):
                        for search_item in v['search_results']:
                            with st.container():
                                st.markdown(f"**ì œëª©:** {search_item['title']}")
                                st.markdown(f"**ê²€ìƒ‰ ì¿¼ë¦¬:** {search_item['search_query']}")
                                st.markdown(f"**URL:** [{search_item['url']}]({search_item['url']})")
                                st.markdown(f"**ë‚´ìš©:** {search_item['content'][:500]}...")
                                        
                                st.markdown("---")
                                    
                       
                elif k == 'generate_response':
                    st.markdown("## ğŸ“‹ ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸")
                    st.markdown(v['answer'])
